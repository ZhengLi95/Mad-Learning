---

typora-copy-images-to: img
---

## é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹MDP

### é©¬å°”å¯å¤«æ¨¡å‹çš„å‡ ç±»å­æ¨¡å‹

å„ç§é©¬å°”å¯å¤«å­æ¨¡å‹çš„å…³ç³»:

|                |     ä¸è€ƒè™‘åŠ¨ä½œ      |              è€ƒè™‘åŠ¨ä½œ               |
| :------------: | :-----------------: | :---------------------------------: |
|  çŠ¶æ€å®Œå…¨å¯è§  |   é©¬å°”ç§‘å¤«é“¾(MC)    |        é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)        |
| çŠ¶æ€ä¸å®Œå…¨å¯è§ | éšé©¬å°”å¯å¤«æ¨¡å‹(HMM) | ä¸å®Œå…¨å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(POMDP) |



### é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹

ä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç”±ä¸€ä¸ªå››å…ƒç»„æ„æˆ$M = (S, A, P_{sa}, ğ‘…)$ [æ³¨1]

- S: è¡¨ç¤ºçŠ¶æ€é›†(states)ï¼Œæœ‰$sâˆˆS$ï¼Œ$s_i$è¡¨ç¤ºç¬¬iæ­¥çš„çŠ¶æ€ã€‚
- A:è¡¨ç¤ºä¸€ç»„åŠ¨ä½œ(actions)ï¼Œæœ‰$aâˆˆA$ï¼Œ$a_i$è¡¨ç¤ºç¬¬iæ­¥çš„åŠ¨ä½œã€‚
- $ğ‘ƒ_{sa}$: è¡¨ç¤ºçŠ¶æ€è½¬ç§»æ¦‚ç‡ã€‚ğ‘ƒsğ‘ è¡¨ç¤ºçš„æ˜¯åœ¨å½“å‰$s âˆˆ S$çŠ¶æ€ä¸‹ï¼Œç»è¿‡$a âˆˆ A$ä½œç”¨åï¼Œä¼šè½¬ç§»åˆ°çš„å…¶ä»–çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒæƒ…å†µã€‚æ¯”å¦‚ï¼Œåœ¨çŠ¶æ€sä¸‹æ‰§è¡ŒåŠ¨ä½œ$a$ï¼Œè½¬ç§»åˆ°mçš„æ¦‚ç‡å¯ä»¥è¡¨ç¤ºä¸º$p(s'|s,a)$ã€‚
- $R: SÃ—AâŸ¼â„$ ï¼ŒRæ˜¯å›æŠ¥å‡½æ•°(reward function)ã€‚æœ‰äº›å›æŠ¥å‡½æ•°çŠ¶æ€$S$çš„å‡½æ•°ï¼Œå¯ä»¥ç®€åŒ–ä¸º$R: SâŸ¼â„$ã€‚å¦‚æœä¸€ç»„$(s,a)$è½¬ç§»åˆ°äº†ä¸‹ä¸ªçŠ¶æ€$s'$ï¼Œé‚£ä¹ˆå›æŠ¥å‡½æ•°å¯è®°ä¸º$r(s'|s, a)$ã€‚å¦‚æœ$(s,a)$å¯¹åº”çš„ä¸‹ä¸ªçŠ¶æ€s'æ˜¯å”¯ä¸€çš„ï¼Œé‚£ä¹ˆå›æŠ¥å‡½æ•°ä¹Ÿå¯ä»¥è®°ä¸º$r(s,a)$ã€‚

MDP çš„åŠ¨æ€è¿‡ç¨‹å¦‚ä¸‹ï¼šæŸä¸ªæ™ºèƒ½ä½“(agent)çš„åˆå§‹çŠ¶æ€ä¸º$s_0$ï¼Œç„¶åä» A ä¸­æŒ‘é€‰ä¸€ä¸ªåŠ¨ä½œ$a_0$æ‰§è¡Œï¼Œæ‰§è¡Œåï¼Œagent æŒ‰$P_sa$æ¦‚ç‡éšæœºè½¬ç§»åˆ°äº†ä¸‹ä¸€ä¸ª$s1$çŠ¶æ€ï¼Œ$s1âˆˆP_{s_0a_0}$ã€‚ç„¶åå†æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œ$a_1$ï¼Œå°±è½¬ç§»åˆ°äº†$s_2$ï¼Œæ¥ä¸‹æ¥å†æ‰§è¡Œ$a_2$â€¦ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„å›¾è¡¨ç¤ºçŠ¶æ€è½¬ç§»çš„è¿‡ç¨‹ã€‚

![img](https://images0.cnblogs.com/blog/489049/201401/131433102201.jpg)

å¦‚æœå›æŠ¥ræ˜¯æ ¹æ®çŠ¶æ€så’ŒåŠ¨ä½œaå¾—åˆ°çš„ï¼Œåˆ™MDPè¿˜å¯ä»¥è¡¨ç¤ºæˆä¸‹å›¾ï¼š

![img](https://images0.cnblogs.com/blog/489049/201401/132312526273.jpg)



### å€¼å‡½æ•°(value function)

å¢å¼ºå­¦ä¹ å­¦åˆ°çš„æ˜¯ä¸€ä¸ªä»ç¯å¢ƒçŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼ˆå³è¡Œä¸ºç­–ç•¥ï¼‰ï¼Œè®°ä¸ºç­–ç•¥Ï€: Sâ†’Aã€‚è€Œå¢å¼ºå­¦ä¹ å¾€å¾€åˆå…·æœ‰å»¶è¿Ÿå›æŠ¥çš„ç‰¹ç‚¹: å¦‚æœåœ¨ç¬¬næ­¥è¾“æ‰äº†æ£‹ï¼Œé‚£ä¹ˆåªæœ‰çŠ¶æ€snå’ŒåŠ¨ä½œanè·å¾—äº†ç«‹å³å›æŠ¥r(sn,an)=-1ï¼Œå‰é¢çš„æ‰€æœ‰çŠ¶æ€ç«‹å³å›æŠ¥å‡ä¸º0ã€‚æ‰€ä»¥å¯¹äºä¹‹å‰çš„ä»»æ„çŠ¶æ€så’ŒåŠ¨ä½œaï¼Œç«‹å³å›æŠ¥å‡½æ•°r(s,a)æ— æ³•è¯´æ˜ç­–ç•¥çš„å¥½åã€‚å› è€Œéœ€è¦å®šä¹‰å€¼å‡½æ•°(value functionï¼Œåˆå«æ•ˆç”¨å‡½æ•°)æ¥<font color=#FF8C00>**è¡¨æ˜å½“å‰çŠ¶æ€ä¸‹ç­–ç•¥Ï€çš„é•¿æœŸå½±å“**</font>ã€‚

1. çŠ¶æ€å€¼å‡½æ•°(state value function)

   - ![img](https://images0.cnblogs.com/blog/489049/201401/171629098615.png)
   - ![img](https://images0.cnblogs.com/blog/489049/201401/171629100809.png)
   - ![img](https://images0.cnblogs.com/blog/489049/201401/171629102528.png)

   å…¶ä¸­ï¼š

   a)æ˜¯é‡‡ç”¨ç­–ç•¥Ï€çš„æƒ…å†µä¸‹æœªæ¥æœ‰é™hæ­¥çš„æœŸæœ›ç«‹å³å›æŠ¥æ€»å’Œï¼›

   b)æ˜¯é‡‡ç”¨ç­–ç•¥Ï€çš„æƒ…å†µä¸‹æœŸæœ›çš„å¹³å‡å›æŠ¥ï¼›

   c)æ˜¯å€¼å‡½æ•°æœ€å¸¸è§çš„å½¢å¼ï¼Œå¼ä¸­Î³âˆˆ[0,1]ç§°ä¸ºæŠ˜åˆå› å­ï¼Œè¡¨æ˜äº†æœªæ¥çš„å›æŠ¥ç›¸å¯¹äºå½“å‰å›æŠ¥çš„é‡è¦ç¨‹åº¦ã€‚ç‰¹åˆ«çš„ï¼ŒÎ³=0æ—¶ï¼Œç›¸å½“äºåªè€ƒè™‘ç«‹å³ä¸è€ƒè™‘é•¿æœŸå›æŠ¥ï¼ŒÎ³=1æ—¶ï¼Œå°†é•¿æœŸå›æŠ¥å’Œç«‹å³å›æŠ¥çœ‹å¾—åŒç­‰é‡è¦ã€‚

   <font color=#FF8C00>**é‡ç‚¹çœ‹ç¬¬ä¸‰ä¸ªå¼å­**</font>

   ![img](https://images0.cnblogs.com/blog/489049/201401/171629104860.png)

   ç»™å®šç­–ç•¥Ï€å’Œåˆå§‹çŠ¶æ€sï¼Œåˆ™åŠ¨ä½œa=Ï€(s)ï¼Œä¸‹ä¸ªæ—¶åˆ»å°†ä»¥æ¦‚ç‡p(s'|s,a)è½¬å‘ä¸‹ä¸ªçŠ¶æ€s'ï¼Œé‚£ä¹ˆä¸Šå¼çš„æœŸæœ›å¯ä»¥æ‹†å¼€ï¼Œå¯ä»¥é‡å†™ä¸ºï¼š

   ![img](https://images0.cnblogs.com/blog/489049/201401/171629106589.png)

   **æ³¨æ„ï¼š**åœ¨$V^Ï€(s)$ä¸­ï¼ŒÏ€å’Œåˆå§‹çŠ¶æ€sæ˜¯æˆ‘ä»¬ç»™å®šçš„ï¼Œè€Œåˆå§‹åŠ¨ä½œaæ˜¯ç”±ç­–ç•¥Ï€å’ŒçŠ¶æ€så†³å®šçš„ï¼Œå³a=Ï€(s)ã€‚

2. åŠ¨ä½œå€¼å‡½æ•°(action value functionQå‡½æ•°)

   ![img](https://images0.cnblogs.com/blog/489049/201401/171629108773.png)

   ç»™å®šå½“å‰çŠ¶æ€så’Œå½“å‰åŠ¨ä½œaï¼Œåœ¨æœªæ¥éµå¾ªç­–ç•¥Ï€ï¼Œé‚£ä¹ˆç³»ç»Ÿå°†ä»¥æ¦‚ç‡p(s'|s,a)è½¬å‘ä¸‹ä¸ªçŠ¶æ€s'ï¼Œä¸Šå¼å¯ä»¥é‡å†™ä¸ºï¼š

   ![img](https://images0.cnblogs.com/blog/489049/201401/171629110338.png)

   <font color=#FF8C00>**åœ¨$Q^Ï€(s,a)$ä¸­ï¼Œä¸ä»…ç­–ç•¥Ï€å’Œåˆå§‹çŠ¶æ€sæ˜¯æˆ‘ä»¬ç»™å®šçš„ï¼Œå½“å‰çš„åŠ¨ä½œaä¹Ÿæ˜¯æˆ‘ä»¬ç»™å®šçš„ï¼Œè¿™æ˜¯$Q^Ï€(s,a)$å’Œ$V^Ï€(a)$çš„ä¸»è¦åŒºåˆ«ã€‚**</font>

åœ¨å¾—åˆ°å€¼å‡½æ•°åï¼Œå³å¯åˆ—å‡ºMDPçš„æœ€ä¼˜ç­–ç•¥ï¼š

![img](https://images0.cnblogs.com/blog/489049/201401/171629112670.png)

å³æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯»æ‰¾çš„æ˜¯åœ¨ä»»æ„åˆå§‹æ¡ä»¶sä¸‹ï¼Œèƒ½å¤Ÿæœ€å¤§åŒ–å€¼å‡½æ•°çš„ç­–ç•¥Ï€*ã€‚

**[ä¸€ä¸ªä¼˜ç§€çš„ä¾‹å­](https://www.cnblogs.com/jinxulin/p/3517377.html)**



## MDPçš„åŠ¨æ€è§„åˆ’è§£æ³•

åŸºæœ¬çš„è§£æ³•æœ‰ä¸‰ç§ï¼š

- åŠ¨æ€è§„åˆ’æ³•(dynamic programming methods)
- è’™ç‰¹å¡ç½—æ–¹æ³•(Monte Carlo methods)
- æ—¶é—´å·®åˆ†æ³•(temporal difference)

æœ¬æ–‡å…ˆä»‹ç»åŠ¨æ€è§„åˆ’æ³•æ±‚è§£MDP

### è´å°”æ›¼æ–¹ç¨‹ï¼ˆBellman Equationï¼‰

$V^\pi$å’Œ$Q^\pi$çš„è¡¨è¾¾å¼æ€»ç»“å¦‚ä¸‹ï¼š

![img](https://images0.cnblogs.com/blog/489049/201401/201019351882.png)

åœ¨åŠ¨æ€è§„åˆ’ä¸­ï¼Œä¸Šé¢ä¸¤ä¸ªå¼å­ç§°ä¸º**è´å°”æ›¼æ–¹ç¨‹**ï¼Œå®ƒè¡¨æ˜äº†<font color=#FF8C00>**å½“å‰çŠ¶æ€çš„å€¼å‡½æ•°ä¸ä¸‹ä¸ªçŠ¶æ€çš„å€¼å‡½æ•°çš„å…³ç³»**  ã€‚</font>

ä¼˜åŒ–ç›®æ ‡$Ï€^*$å¯ä»¥è¡¨ç¤ºä¸ºï¼š

![img](https://images0.cnblogs.com/blog/489049/201401/201019355785.png)

åˆ†åˆ«è®°æœ€ä¼˜ç­–ç•¥$Ï€$å¯¹åº”çš„çŠ¶æ€å€¼å‡½æ•°å’ŒåŠ¨ä½œå€¼å‡½æ•°ä¸º $V^*(s)$ $å’Œ$$Q^*(s, a)$

çŠ¶æ€å€¼å‡½æ•°å’Œè¡Œä¸ºå€¼å‡½æ•°åˆ†åˆ«æ»¡è¶³å¦‚ä¸‹<font color=#FF8C00>**è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹(Bellman optimality equation)**</font>ï¼š

![img](https://images0.cnblogs.com/blog/489049/201401/201019379691.png)

![img](https://images0.cnblogs.com/blog/489049/201401/201019398446.png)

æ•…å¯çŸ¥ï¼Œ$V^*(s)$ $å’Œ$$Q^*(s, a)$å­˜åœ¨å¦‚ä¸‹å…³ç³»ï¼š

![img](https://images0.cnblogs.com/blog/489049/201401/201019377660.png)

### ç­–ç•¥ä¼°è®¡(Policy Evaluation)

å¯¹äºä»»æ„çš„ç­–ç•¥Ï€ï¼Œæˆ‘ä»¬å¦‚ä½•è®¡ç®—å…¶çŠ¶æ€å€¼å‡½æ•°$V^Ï€(s)$ï¼Ÿ

**ç¡®å®šæ€§ç­–ç•¥**ï¼š![img](https://images0.cnblogs.com/blog/489049/201401/201019402195.png)ï¼ˆä¸Šä¸€èŠ‚ä¸»è¦ä»‹ç»çš„å°±æ˜¯ç¡®å®šæ€§ç­–ç•¥ï¼‰

**æ‰©å±•åˆ°ä¸€èˆ¬**ï¼šå¦‚æœåœ¨æŸç­–ç•¥Ï€ä¸‹ï¼ŒÏ€(s)å¯¹åº”çš„åŠ¨ä½œaæœ‰å¤šç§å¯èƒ½ï¼Œæ¯ç§å¯èƒ½è®°ä¸ºÏ€(a|s)ï¼Œåˆ™çŠ¶æ€å€¼å‡½æ•°ä¸º![img](https://images0.cnblogs.com/blog/489049/201401/201019409535.png)

ä¸€èˆ¬é‡‡ç”¨è¿­ä»£çš„æ–¹æ³•æ›´æ–°çŠ¶æ€å€¼å‡½æ•°ï¼Œé¦–å…ˆå°†æ‰€æœ‰VÏ€(s)çš„åˆå€¼èµ‹ä¸º0ï¼ˆå…¶ä»–çŠ¶æ€ä¹Ÿå¯ä»¥èµ‹ä¸ºä»»æ„å€¼ï¼Œä¸è¿‡å¸æ”¶æ€å¿…é¡»èµ‹0å€¼ï¼‰ï¼Œç„¶åé‡‡ç”¨å¦‚ä¸‹å¼å­æ›´æ–°æ‰€æœ‰çŠ¶æ€sçš„å€¼å‡½æ•°ï¼ˆç¬¬k+1æ¬¡è¿­ä»£ï¼‰ï¼š

![img](https://images0.cnblogs.com/blog/489049/201401/201019411259.png)

å¯¹äº$V^Ï€(s)$ï¼Œæœ‰**ä¸¤ç§æ›´æ–°æ–¹æ³•**ï¼š

1. å°†ç¬¬kæ¬¡è¿­ä»£çš„å„çŠ¶æ€å€¼å‡½æ•° $[V_k(s1),V_k(s2),V_k(s3), \dots]$ ä¿å­˜åœ¨ä¸€ä¸ªæ•°ç»„ä¸­ï¼Œç¬¬k+1æ¬¡çš„VÏ€(s)é‡‡ç”¨ç¬¬kæ¬¡çš„$V^Ï€(s')$æ¥è®¡ç®—ï¼Œå¹¶å°†ç»“æœä¿å­˜åœ¨ç¬¬äºŒä¸ªæ•°ç»„ä¸­ã€‚
2. å³ä»…ç”¨ä¸€ä¸ªæ•°ç»„ä¿å­˜å„çŠ¶æ€å€¼å‡½æ•°ï¼Œæ¯å½“å¾—åˆ°ä¸€ä¸ªæ–°å€¼ï¼Œå°±**å°†æ—§çš„å€¼è¦†ç›–**,å½¢å¦‚$[V_{k+1}(s1),V_{k+1}(s2),V_k(s3), \dots]$ï¼Œç¬¬k+1æ¬¡è¿­ä»£çš„$V^Ï€(s)$å¯èƒ½ç”¨åˆ°ç¬¬k+1æ¬¡è¿­ä»£å¾—åˆ°çš„$V^Ï€(s')$ã€‚



é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬**é‡‡ç”¨ç¬¬äºŒç§æ–¹æ³•æ›´æ–°æ•°æ®**ï¼Œå› ä¸ºå®ƒåŠæ—¶åˆ©ç”¨äº†æ–°å€¼ï¼Œèƒ½æ›´å¿«çš„æ”¶æ•›ã€‚æ•´ä¸ªç­–ç•¥ä¼°è®¡ç®—æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![img](https://images0.cnblogs.com/blog/489049/201401/201019414696.png)

### ç­–ç•¥æ”¹è¿›(Policy Improvement) (ç­–ç•¥æ‹©ä¼˜)

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç­–ç•¥$Ï€$ï¼Œå¹¶ä¸”ç¡®å®šäº†å®ƒçš„æ‰€æœ‰çŠ¶æ€çš„å€¼å‡½æ•°$V^Ï€(s)$ã€‚å¯¹äºæŸçŠ¶æ€sï¼Œæœ‰åŠ¨ä½œ$a_0=Ï€(s)$ã€‚ é‚£ä¹ˆå¦‚æœæˆ‘ä»¬åœ¨çŠ¶æ€sä¸‹ä¸é‡‡ç”¨åŠ¨ä½œ$a_0$ï¼Œè€Œé‡‡ç”¨å…¶ä»–åŠ¨ä½œ$aâ‰ Ï€(s)$æ˜¯å¦ä¼šæ›´å¥½å‘¢ï¼Ÿè¦åˆ¤æ–­å¥½åå°±éœ€è¦æˆ‘ä»¬è®¡ç®—è¡Œä¸ºå€¼å‡½æ•°$Q^Ï€(s,a)$ï¼Œå…¬å¼æˆ‘ä»¬å‰é¢å·²ç»è¯´è¿‡ï¼š

![img](https://images0.cnblogs.com/blog/489049/201401/201019420000.png)

<font color=#FF8C00>**è¯„åˆ¤æ ‡å‡†**</font>æ˜¯ï¼š$Q^Ï€(s,a)$æ˜¯å¦å¤§äº$V^Ï€(s)$ã€‚å¦‚æœ$Q^Ï€(s,a)$> $V^Ï€(s)$ï¼Œé‚£ä¹ˆè‡³å°‘è¯´æ˜æ–°ç­–ç•¥ã€ä»…åœ¨çŠ¶æ€sä¸‹é‡‡ç”¨åŠ¨ä½œaï¼Œå…¶ä»–çŠ¶æ€ä¸‹éµå¾ªç­–ç•¥Ï€ã€‘æ¯”æ—§ç­–ç•¥ã€æ‰€æœ‰çŠ¶æ€ä¸‹éƒ½éµå¾ªç­–ç•¥Ï€ã€‘æ•´ä½“ä¸Šè¦æ›´å¥½ã€‚

<font color=#FF8C00>**ç­–ç•¥æ”¹è¿›å®šç†(policy improvement theorem)**</font>ï¼š$Ï€$å’Œ$Ï€'$æ˜¯ä¸¤ä¸ªç¡®å®šçš„ç­–ç•¥ï¼Œå¦‚æœå¯¹æ‰€æœ‰çŠ¶æ€$sâˆˆS$æœ‰$Q^Ï€(s,Ï€'(s))â‰¥V^Ï€(s)$ï¼Œé‚£ä¹ˆç­–ç•¥Ï€'å¿…ç„¶æ¯”ç­–ç•¥Ï€æ›´å¥½ï¼Œæˆ–è€…è‡³å°‘ä¸€æ ·å¥½ã€‚å…¶ä¸­çš„ä¸ç­‰å¼ç­‰ä»·äº$V^{Ï€'}(s)â‰¥V^Ï€(s)$ã€‚

æœ‰äº†åœ¨æŸçŠ¶æ€sä¸Šæ”¹è¿›ç­–ç•¥çš„æ–¹æ³•å’Œç­–ç•¥æ”¹è¿›å®šç†ï¼Œæˆ‘ä»¬å¯ä»¥éå†æ‰€æœ‰çŠ¶æ€å’Œæ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œaï¼Œå¹¶é‡‡ç”¨è´ªå¿ƒç­–ç•¥æ¥è·å¾—æ–°ç­–ç•¥$Ï€'$ã€‚å³å¯¹æ‰€æœ‰çš„$sâˆˆS$, é‡‡ç”¨ä¸‹å¼æ›´æ–°ç­–ç•¥ï¼š

![img](https://images0.cnblogs.com/blog/489049/201401/201019423289.png)

è¿™ç§é‡‡ç”¨å…³äºå€¼å‡½æ•°çš„è´ªå¿ƒç­–ç•¥è·å¾—æ–°ç­–ç•¥ï¼Œæ”¹è¿›æ—§ç­–ç•¥çš„è¿‡ç¨‹ï¼Œç§°ä¸ºç­–ç•¥æ”¹è¿›(Policy Improvement)

è´ªå¿ƒç­–ç•¥æ”¶æ•›:

> å‡è®¾ç­–ç•¥æ”¹è¿›è¿‡ç¨‹å·²ç»æ”¶æ•›ï¼Œå³å¯¹æ‰€æœ‰çš„sï¼ŒVÏ€'(s)ç­‰äºVÏ€(s)ã€‚é‚£ä¹ˆæ ¹æ®ä¸Šé¢çš„ç­–ç•¥æ›´æ–°çš„å¼å­ï¼Œå¯ä»¥çŸ¥é“å¯¹äºæ‰€æœ‰çš„sâˆˆSä¸‹å¼æˆç«‹:
>
> ![img](https://images0.cnblogs.com/blog/489049/201401/201019434382.png)

è¿™ä¸ªå¼å­æ­£å¥½å°±æ˜¯æˆ‘ä»¬åœ¨1ä¸­æ‰€è¯´çš„Bellman optimality equationï¼Œæ‰€ä»¥Ï€å’ŒÏ€'éƒ½å¿…ç„¶æ˜¯æœ€ä¼˜ç­–ç•¥ï¼ç¥å¥‡å§ï¼





## å¼ºåŒ–å­¦ä¹ 

### åˆ†ç±»

1. - ä¸ç†è§£ç¯å¢ƒ(Model-free RL)

     Q Learningã€Sarsaã€Policy Gradients

   - ç†è§£ç¯å¢ƒ(Model-Based RL)

     ç›¸è¾ƒäºModel-freeï¼ŒModel-Basedå¯ä»¥æ„å»ºè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶ä¸”å¯ä»¥**æƒ³è±¡**

2. - åŸºäºæ¦‚ç‡(Policy-Based RL)ï¼šè¾“å‡ºçš„æ˜¯æ¦‚ç‡ï¼Œä¸ä¸€å®šé€‰æ‹©æœ€é«˜æ¦‚ç‡çš„ï¼Œé€‚ç”¨äºè¿ç»­åŠ¨ä½œï¼šPolicy Gradients
- åŸºäºä»·å€¼(Value-Based RL)ï¼šè¾“å‡ºçš„æ˜¯ä»·å€¼ï¼Œä¸€å®šé€‰æ‹©ä»·å€¼æœ€é«˜çš„åŠ¨ä½œï¼šQLearningã€Sarsa
   - Actor-Criticï¼šActoråŸºäºæ¦‚ç‡åšå‡ºåŠ¨ä½œï¼ŒCriticå†åŸºäºä»·å€¼ç»™å‡ºåŠ¨ä½œçš„ä»·å€¼
   

3. - å›åˆæ›´æ–°(Monte-Carlo update)ï¼šæ¸¸æˆç»“æŸæ›´æ–°

     åŸºç¡€ç‰ˆPolicy Gradients

   - å•æ­¥æ›´æ–°(Temporal-Difference update)ï¼šæ¸¸æˆè¿›è¡Œçš„æ¯ä¸€æ­¥éƒ½å¯ä»¥æ›´æ–°

     Q Learningã€Sarsaã€å‡çº§ç‰ˆPolicy Gradients

4. - åœ¨çº¿å­¦ä¹ (On-Policy)

     Sarsaã€Sarsa($\lambda$)

   - ç¦»çº¿å­¦ä¹ (Off-Policy)

     Q Learningã€DQN

### Q Learning 

æ ¹æ®$Q$è¡¨å¯¹ä¸‹ä¸€æ—¶åˆ»çš„åŠ¨ä½œè¿›è¡Œé€‰æ‹©ï¼Œä¸‹å›¾æ˜¯$Q$è¡¨çš„æ›´æ–°æ–¹å¼

![](img\2019-04-10 19-14-32 çš„å±å¹•æˆªå›¾.png)

æ­¤æ—¶ï¼Œ$S_2$å¹¶æœªè¿›è¡Œä¸‹ä¸€æ¬¡çš„åŠ¨ä½œï¼Œè€Œæ˜¯é¢„ä¼°äº†ä¸€ä¸‹åæœï¼Œç”±æ­¤æ¥æ›´æ–°$S_1$çš„$Q$è¡¨ã€‚

![](img\2019-04-10 19-17-30 çš„å±å¹•æˆªå›¾.png)

å…¶ä¸­ï¼Œ$\alpha$æ˜¯**å­¦ä¹ é€Ÿç‡**ï¼Œ$\epsilon$æ˜¯**é€‰æ‹©$Q$è¡¨æœ€å¤§å€¼çš„æ¦‚ç‡**ã€‚è‹¥$\epsilon=90\%$ï¼Œåˆ™$90\%$æ¦‚ç‡é€‰æ‹©$Q$è¡¨æœ€å¤§å€¼å³æœ€ä¼˜åŠ¨ä½œï¼Œ$10\%$çš„æ¦‚ç‡éšæœºåŠ¨ä½œã€‚

ç”±äº$Q(s',a')$æ˜¯ä¸‹ä¸€æ¬¡çš„åŠ¨ä½œï¼Œä¼šé€šè¿‡ä¹˜ä»¥**å¥–åŠ±è¡°å‡å€¼**$\gamma$çš„æ–¹å¼å½±å“å‰ä¸€æ¬¡çš„$Q$è¡¨å–å€¼ï¼Œå› æ­¤å¾ˆå®¹æ˜“æƒ³åˆ°åªè¦$\gamma\neq 0$ï¼Œä»¥åçš„æ¯æ¬¡åŠ¨ä½œå¾—åˆ°çš„å¥–åŠ±å€¼éƒ½ä¼šå½±å“ä¹‹å‰åŠ¨ä½œçš„$Q$è¡¨å–å€¼ã€‚

- Qä¼°è®¡ï¼š$s_1$çŠ¶æ€æœ€ä¼˜åŠ¨ä½œ$a$çš„Qå€¼
- Qç°å®ï¼šåœ¨é€‰æ‹©äº†åŠ¨ä½œ$a$åï¼Œè¿›å…¥ $s'$çŠ¶æ€ã€‚Qè¡¨ä¸­ $s'$çŠ¶æ€å¯¹åº”çš„Qå€¼çš„æœ€å¤§å€¼åŠ ä¸Šæ‰§è¡ŒåŠ¨ä½œ$a$ä¹‹åå¾—åˆ°çš„å¥–åŠ±å€¼$r$ï¼Œå³
- ä¸ºQç°å®ã€‚

### Sarsa

1. å†³ç­–éƒ¨åˆ†ä¸Q Learningä¸€æ ·
2. åŒºåˆ«åœ¨äºQè¡¨çš„æ›´æ–°æ–¹å¼ï¼š
   - Q Learningå¹¶æ²¡æœ‰å®é™…è¿›è¡Œä¸‹ä¸€æ¬¡çš„åŠ¨ä½œï¼Œ**ä¼šé€‰æ‹©æœ‰å±é™©çš„åŠ¨ä½œ**ï¼Œå› æ­¤æ˜¯ç¦»çº¿å­¦ä¹ ï¼›
   - Sarsaæ˜¯å®è·µæ´¾ï¼Œè¿›è¡Œäº†å®é™…çš„ä¸‹ä¸€æ­¥åŠ¨ä½œï¼Œ**ä¼šé¿å…é€‰æ‹©æœ‰å±é™©çš„åŠ¨ä½œ**ï¼Œå› æ­¤æ˜¯åœ¨çº¿å­¦ä¹ ï¼›

![](img\2019-04-10 21-00-18 çš„å±å¹•æˆªå›¾.png)



### Sarsa($\lambda$)

1. **ä¸ä»…æ›´æ–°ç¦»å¥–åŠ±æœ€è¿‘çš„ä¸€æ­¥ï¼Œè¿˜æ›´æ–°æ¥æ—¶æ²¿é€”çš„æ¯ä¸€æ­¥**

   > Sarsa æ˜¯ä¸€ç§å•æ­¥æ›´æ–°æ³•, åœ¨ç¯å¢ƒä¸­æ¯èµ°ä¸€æ­¥, æ›´æ–°ä¸€æ¬¡è‡ªå·±çš„è¡Œä¸ºå‡†åˆ™, æˆ‘ä»¬å¯ä»¥åœ¨è¿™æ ·çš„ Sarsa åé¢æ‰“ä¸€ä¸ªæ‹¬å·, è¯´ä»–æ˜¯ Sarsa(0), å› ä¸ºä»–ç­‰èµ°å®Œè¿™ä¸€æ­¥ä»¥åç›´æ¥æ›´æ–°è¡Œä¸ºå‡†åˆ™. å¦‚æœå»¶ç»­è¿™ç§æƒ³æ³•, èµ°å®Œè¿™æ­¥, å†èµ°ä¸€æ­¥, ç„¶åå†æ›´æ–°, æˆ‘ä»¬å¯ä»¥å«ä»– Sarsa(1). åŒç†, å¦‚æœç­‰å¾…å›åˆå®Œæ¯•æˆ‘ä»¬ä¸€æ¬¡æ€§å†æ›´æ–°å‘¢, æ¯”å¦‚è¿™å›åˆæˆ‘ä»¬èµ°äº† n æ­¥, é‚£æˆ‘ä»¬å°±å« Sarsa(n). ä¸ºäº†ç»Ÿä¸€è¿™æ ·çš„æµç¨‹, æˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ª $\lambda$ å€¼æ¥ä»£æ›¿æˆ‘ä»¬æƒ³è¦é€‰æ‹©çš„æ­¥æ•°, è¿™ä¹Ÿå°±æ˜¯ Sarsa($\lambda$) çš„ç”±æ¥.Â 

2. æ›´æ–°æ²¿é€”å­˜åœ¨çš„**é—®é¢˜**ï¼šä¼šè®°å½•ä¸å¿…è¦çš„é‡å¤æ­¥éª¤ï¼Œå› æ­¤éœ€è¦Sarsa($\lambda$)

3. $\lambda$ çš„**å«ä¹‰**ï¼š

   ![Sarsa(img\sl4.png)](https://morvanzhou.github.io/static/results/ML-intro/sl4.png)

   $\lambda$æ˜¯ä¸€ä¸ª**è¡°å˜å€¼**ï¼Œåœ¨$0-1$ä¹‹é—´ï¼Œç¦»å¥–åŠ±è¶Šè¿œçš„æ­¥å¯¹åˆ°è¾¾å¥–åŠ±çš„å½±å“è¶Šå°ï¼Œè¡°å‡è¶Šå¤§ï¼›åä¹‹ï¼Œç¦»å¥–åŠ±è¶Šè¿‘çš„æ­¥å¯¹å¥–åŠ±çš„å½±å“è¶Šå¤§ï¼Œè¡°å‡è¾ƒå°ã€‚

   ![Sarsa-lambda](img\3-3-2.png)

4. **ç®—æ³•**ï¼š

![Sarsa-lambda](img\3-3-1.png)

### DQN(Deep Q Network)

1. æŠ›å¼ƒQè¡¨è¿™ç§Qå€¼è®°å½•æ–¹å¼ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œç”ŸæˆQå€¼ï¼Œåœ¨çŠ¶æ€è¾ƒå¤šçš„æƒ…å†µä¸‹æ ¼å¤–æœ‰æ•ˆç‡

![DQN](img\DQN3.png)

2. - Qä¼°è®¡ï¼šé€šè¿‡NNé¢„æµ‹å‡ºçš„$Q(s_2, a_1), Q(s_2,a_2)$çš„æœ€å¤§å€¼
   - Qç°å®ï¼šQ ä¼°è®¡ä¸­æœ€å¤§å€¼çš„åŠ¨ä½œæ¥æ¢å–ç¯å¢ƒä¸­çš„å¥–åŠ± reward+$\gamma*$ä¸‹ä¸€æ­¥$s'$ä¸­é€šè¿‡NNé¢„æµ‹å‡ºçš„$Q(sâ€˜, a_1), Q(s',a_2)$çš„æœ€å¤§å€¼
3. **DQNä¸¤å¤§åˆ©å™¨**ï¼š
   - Experience replay: ä½œä¸ºä¸€ç§ç¦»çº¿å­¦ä¹ ï¼Œæ¯æ¬¡ DQN æ›´æ–°çš„æ—¶å€™ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥éšæœºæŠ½å–ä¸€äº›ä¹‹å‰çš„ç»å†è¿›è¡Œå­¦ä¹ ã€‚éšæœºæŠ½å–è¿™ç§åšæ³•æ‰“ä¹±äº†ç»å†ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä¹Ÿä½¿å¾—ç¥ç»ç½‘ç»œæ›´æ–°æ›´æœ‰æ•ˆç‡ã€‚
   - Fixed Q-target: åœ¨ DQN ä¸­ä½¿ç”¨åˆ°ä¸¤ä¸ªç»“æ„ç›¸åŒä½†å‚æ•°ä¸åŒçš„ç¥ç»ç½‘ç»œ, é¢„æµ‹ Q ä¼°è®¡çš„ç¥ç»ç½‘ç»œå…·å¤‡æœ€æ–°çš„å‚æ•°, è€Œé¢„æµ‹ Q ç°å®çš„ç¥ç»ç½‘ç»œä½¿ç”¨çš„å‚æ•°åˆ™æ˜¯å¾ˆä¹…ä»¥å‰çš„.
4. **ç®—æ³•**ï¼š
![DQN ç®—æ³•æ›´æ–° (img\4-1-1-1554948278323.jpg)](https://morvanzhou.github.io/static/results/reinforcement-learning/4-1-1.jpg)

- è®°å¿†åº“ (ç”¨äºé‡å¤å­¦ä¹ )
- ç¥ç»ç½‘ç»œè®¡ç®— Q å€¼
- æš‚æ—¶å†»ç»“Â `q_target`Â å‚æ•° (åˆ‡æ–­ç›¸å…³æ€§)



### Double DQN

å› ä¸ºæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œé¢„æµ‹ `Qmax` æœ¬æ¥å°±æœ‰è¯¯å·®, æ¯æ¬¡ä¹Ÿå‘ç€æœ€å¤§è¯¯å·®çš„ `Qç°å®` æ”¹è¿›ç¥ç»ç½‘ç»œ, å°±æ˜¯å› ä¸ºè¿™ä¸ª `Qmax` å¯¼è‡´äº† overestimate. æ‰€ä»¥ Double DQN çš„æƒ³æ³•å°±æ˜¯å¼•å…¥å¦ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥æ‰“æ¶ˆä¸€äº›æœ€å¤§è¯¯å·®çš„å½±å“. è€Œ DQN ä¸­æœ¬æ¥å°±æœ‰ä¸¤ä¸ªç¥ç»ç½‘ç»œ, æˆ‘ä»¬ä½•ä¸åˆ©ç”¨ä¸€ä¸‹è¿™ä¸ªåœ°ç†ä¼˜åŠ¿å‘¢. æ‰€ä»¥, æˆ‘ä»¬ç”¨ `Qä¼°è®¡` çš„ç¥ç»ç½‘ç»œä¼°è®¡ `Qç°å®` ä¸­ `Qmax(s', a')` çš„æœ€å¤§åŠ¨ä½œå€¼. ç„¶åç”¨è¿™ä¸ªè¢«`Qä¼°è®¡` ä¼°è®¡å‡ºæ¥çš„åŠ¨ä½œæ¥é€‰æ‹© `Qç°å®` ä¸­çš„ `Q(s')`. æ€»ç»“ä¸€ä¸‹:

æœ‰ä¸¤ä¸ªç¥ç»ç½‘ç»œ: `Q_eval` (Qä¼°è®¡ä¸­çš„), `Q_next` (Qç°å®ä¸­çš„).

åŸæœ¬çš„ `Q_next = max(Q_next(s', a_all))`.

Double DQN ä¸­çš„ `Q_next = Q_next(s', argmax(Q_eval(s', a_all)))`. ä¹Ÿå¯ä»¥è¡¨è¾¾æˆä¸‹é¢é‚£æ ·.



å¯¹æ¯”åŸå§‹å’ŒDoubleçš„costæ›²çº¿ï¼š

![Double DQN (./img/4-5-4.png)](https://morvanzhou.github.io/static/results/reinforcement-learning/4-5-4.png)

### Policy gradient

Policy gradientè¾“å‡ºä¸æ˜¯ action çš„ value, è€Œæ˜¯å…·ä½“çš„é‚£ä¸€ä¸ª action, è¿™æ · policy gradient å°±è·³è¿‡äº† value è¿™ä¸ªé˜¶æ®µ.

**ä¼˜åŠ¿ï¼š**è¾“å‡ºçš„è¿™ä¸ª action å¯ä»¥æ˜¯ä¸€ä¸ª**è¿ç»­å€¼**, ä¹‹å‰æˆ‘ä»¬è¯´åˆ°çš„ value-based æ–¹æ³•è¾“å‡ºçš„éƒ½æ˜¯ä¸è¿ç»­çš„å€¼, ç„¶åå†é€‰æ‹©å€¼æœ€å¤§çš„ action. è€Œ policy gradient å¯ä»¥åœ¨ä¸€ä¸ªè¿ç»­åˆ†å¸ƒä¸Šé€‰å– action.

#### ç®—æ³•

ä¸€ç§åŸºäºæ•´æ¡å›åˆæ•°æ®çš„æ›´æ–°

![Policy Gradients ç®—æ³•æ›´æ–° (./img/5-1-1.png)](https://morvanzhou.github.io/static/results/reinforcement-learning/5-1-1.png)

å…¶ä¸­ï¼Œ$\nabla log \pi_{\theta}(s_t,a_t)v_t$è¡¨ç¤ºåœ¨çŠ¶æ€ $s$å¯¹æ‰€é€‰åŠ¨ä½œçš„ $a$çš„åƒæƒŠåº¦ï¼Œ$\pi_{\theta}(s_t,a_t)$ä»£è¡¨ $Policy(s,a)$ï¼Œå…¶æ¦‚ç‡è¶Šå°ï¼Œåå‘çš„ $log(Policy(s,a))$(å³ `-log(P)`) åè€Œè¶Šå¤§. å¦‚æœåœ¨ `Policy(s,a)` å¾ˆå°çš„æƒ…å†µä¸‹, æ‹¿åˆ°äº†ä¸€ä¸ª å¤§çš„ `R`, ä¹Ÿå°±æ˜¯å¤§çš„ `V`, é‚£$\nabla log \pi_{\theta}(s_t,a_t)v_t$  å°±æ›´å¤§, è¡¨ç¤ºæ›´åƒæƒŠ, (**æˆ‘é€‰äº†ä¸€ä¸ªä¸å¸¸é€‰çš„åŠ¨ä½œ, å´å‘ç°åŸæ¥å®ƒèƒ½å¾—åˆ°äº†ä¸€ä¸ªå¥½çš„ reward, é‚£æˆ‘å°±å¾—å¯¹æˆ‘è¿™æ¬¡çš„å‚æ•°è¿›è¡Œä¸€ä¸ªå¤§å¹…ä¿®æ”¹**). è¿™å°±æ˜¯åƒæƒŠåº¦çš„ç‰©ç†æ„ä¹‰ã€‚

### Actor Critic

ç»“åˆäº† Policy Gradient (Actor) å’Œ Function Approximation (Critic) çš„æ–¹æ³•. `Actor` åŸºäºæ¦‚ç‡é€‰è¡Œä¸º, `Critic` åŸºäº `Actor` çš„è¡Œä¸ºè¯„åˆ¤è¡Œä¸ºçš„å¾—åˆ†, `Actor` æ ¹æ® `Critic` çš„è¯„åˆ†ä¿®æ”¹é€‰è¡Œä¸ºçš„æ¦‚ç‡ï¼Œè¾“å…¥çš„å•æ¬¡å¥–èµå˜æˆäº†criticè¾“å‡ºçš„æ€»å¥–èµå¢é‡td-errorã€‚criticå»ºç«‹s-Qçš„ç½‘ç»œï¼Œç„¶åæ ¹æ®[s, r, s_]æ¥è®­ç»ƒï¼Œå¹¶è¿”å›td-errorã€‚

**ä¼˜åŠ¿ï¼š**å¯ä»¥è¿›è¡Œå•æ­¥æ›´æ–°, æ¯”ä¼ ç»Ÿçš„ Policy Gradient è¦å¿«.

**åŠ£åŠ¿ï¼š**å–å†³äº Critic çš„ä»·å€¼åˆ¤æ–­, ä½†æ˜¯ Critic éš¾æ”¶æ•›, å†åŠ ä¸Š Actor çš„æ›´æ–°, å°±æ›´éš¾æ”¶æ•›. ä¸ºäº†è§£å†³æ”¶æ•›é—®é¢˜, Google Deepmind æå‡ºäº† `Actor Critic` å‡çº§ç‰ˆ `Deep Deterministic Policy Gradient`. åè€…èåˆäº† DQN çš„ä¼˜åŠ¿, è§£å†³äº†æ”¶æ•›éš¾çš„é—®é¢˜. 

### DDPG(Deep Deterministic Policy Gradient )

`DDPG` ç»“åˆäº†ä¹‹å‰è·å¾—æˆåŠŸçš„ `DQN` ç»“æ„, æé«˜äº† `Actor Critic` çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚ä¸ºäº†ä½“ç°`DQN`çš„æ€æƒ³ï¼Œæ¯ç§ç¥ç»ç½‘ç»œéƒ½éœ€è¦å†ç»†åˆ†ä¸ºä¸¤ä¸ªï¼Œ 

- Actoræœ‰ä¼°è®¡ç½‘ç»œå’Œç°å®ç½‘ç»œï¼Œä¼°è®¡ç½‘ç»œç”¨æ¥è¾“å‡ºå®æ—¶çš„åŠ¨ä½œ, ä¾›actoråœ¨ç°å®ä¸­å®è¡Œã€‚è€Œç°å®ç½‘ç»œåˆ™æ˜¯ç”¨æ¥æ›´æ–°ä»·å€¼ç½‘ç»œç³»ç»Ÿçš„ã€‚
- Criticè¿™è¾¹ä¹Ÿæœ‰ç°å®ç½‘ç»œå’Œä¼°è®¡ç½‘ç»œï¼Œä»–ä»¬éƒ½åœ¨è¾“å‡ºè¿™ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼Œè€Œè¾“å…¥ç«¯å´æœ‰ä¸åŒï¼ŒçŠ¶æ€ç°å®ç½‘ç»œè¿™è¾¹ä¼šæ‹¿ç€ä»åŠ¨ä½œç°å®ç½‘ç»œæ¥çš„åŠ¨ä½œåŠ ä¸ŠçŠ¶æ€çš„è§‚æµ‹å€¼åŠ ä»¥åˆ†æï¼Œè€ŒçŠ¶æ€ä¼°è®¡ç½‘ç»œåˆ™æ˜¯æ‹¿ç€å½“æ—¶Actoræ–½åŠ çš„åŠ¨ä½œå½“ä½œè¾“å…¥ã€‚

#### ç®—æ³•

![1558614556514](./img/1558614556514.png)



## è®ºæ–‡

### CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING

æœ¬ç¯‡æ–‡ç« æå‡ºäº†åŸºäºDPG(deterministic policy gradient)å’ŒDQNçš„**DDPG**ç®—æ³•ã€‚æˆåŠŸåœ°å°†DQNåº”ç”¨åˆ°**è¿ç»­åŠ¨ä½œ**åŸŸï¼Œå¹¶ä¸”è¿™æ˜¯ä¸€ç§åŸºäºDPGçš„Actor-Criticã€**æ— æ¨¡å‹**ç®—æ³•ã€‚DDPGæ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„ç®—æ³•ï¼Œå¯ä»¥ç›´æ¥ä»åŸå§‹è¾“å…¥æ•°æ®æ˜ å°„åˆ°è¾“å‡ºæ•°æ®ã€‚A long-standing challenge of robotic control is to learn an action policy directly from raw sensory input such as video.

**DQNç¼ºç‚¹ï¼š**

DQNæ˜¯å¯ä»¥è§£å†³çš„é—®é¢˜ä¸€èˆ¬è¦å…·æœ‰é«˜ç»´è§‚æµ‹ç©ºé—´ï¼Œå¹¶ä¸”å…¶åŠ¨ä½œç©ºé—´æ˜¯ä½ç»´ä¸”ç¦»æ•£çš„ã€‚

**DQNä¼˜åŠ¿ï¼š**

1. ç½‘ç»œæ˜¯ä»replay bufferä¸­ç¦»çº¿çš„å­¦ä¹ åˆ°çš„ï¼Œè¿™æ ·å°±å‡å°äº†æ•°æ®é—´çš„ç›¸å…³æ€§ï¼›
2. è¯¥ç½‘ç»œä½¿ç”¨target-Qç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œä»¥ä¾¿åœ¨æ—¶é—´å·®å¼‚å¤‡ä»½æœŸé—´æä¾›ä¸€è‡´çš„ç›®æ ‡ã€‚

**DDPGä¼˜åŠ¿ï¼š**

å¯ä»¥åœ¨ä¿æŒè¶…å‚æ•°å’Œç½‘ç»œç»“æ„ä¸å˜çš„æƒ…å†µä¸‹ï¼Œä»ä½ç»´è§‚æµ‹ç©ºé—´ä¸­é²æ£’åœ°å­¦ä¹ åˆ°ç­–ç•¥ã€‚

#### RLåŸºç¡€

![1558592857137](./img/1558592857137.png)

1. Action-value functionï¼š

    $$
    Q^Ï€(s_t, a_t) = E_{r_{iâ‰¥t},s_{i>t}âˆ¼E,a_{i>t}âˆ¼Ï€} [R_t|s_t, a_t]
    $$

    è¡¨ç¤ºåœ¨çŠ¶æ€ $s_t$ é€‰æ‹©äº†åŠ¨ä½œ $a_t$ çš„æœŸæœ›è¿”å›å€¼ï¼Œå¹¶ä¸”åœ¨æ­¤ä¹‹åç­–ç•¥æœä» $\pi$ã€‚
    
2. è´å°”æ›¼æ–¹ç¨‹ä¸­ä½“ç°äº†çŠ¶æ€ä¹‹é—´çš„é€’å½’å…³ç³»

    $$
    Q^Ï€(s_t, a_t) = E_{r_t,s_{t+1}âˆ¼E} [r(s_t, a_t) + Î³ E_{a_t+1âˆ¼Ï€} [Q^Ï€(s_{t+1}, a_{t+1})]]
    $$

3. å¦‚æœç­–ç•¥æ˜¯ç¡®å®šçš„ï¼Œå¯ç”¨ $\mu : S â† A$ æ¥è¡¨ç¤ºç­–ç•¥ï¼Œå¹¶ä¸”è¿™æ ·å°±çœå»äº†æ±‚ $n+1$ æ¬¡æœŸæœ›çš„è¿‡ç¨‹
    $$
    Q^Âµ(s_t, a_t) = E_{r_t,s_{t+1}âˆ¼E} [r(s_t, a_t) + Î³Q^Âµ(s_{t+1}, Âµ(s_{t+1}))]
    $$
    è¿™æ ·æœŸæœ›å°±åªå’Œç¯å¢ƒæœ‰å…³ï¼Œæ„å‘³ç€å¯ä»¥é€šè¿‡è¿ç§»å­¦ä¹ çš„æ–¹å¼ä»å¦ä¸€ä¸ªéšæœºç­–ç•¥ $\beta$ ä¸­ç¦»çº¿å­¦ä¹ åˆ° $Q^{\mu}$. Q-Leaningæ˜¯ä¸€ç§ç»å…¸ç¦»çº¿å­¦ä¹ ç®—æ³•ï¼Œä½¿ç”¨çš„æ˜¯è´ªå©ªç­–ç•¥ $\mu(s)=\arg \max_aQ(s,a)$
    
4. DPGï¼šThe DPG algorithm maintains a parameterized **actor function Âµ(s|Î¸Âµ)** which specifies the current policy by deterministically mapping states to a specific action. The **critic Q(s, a)** is learned using the Bellman equation as in Q-learning. The actor is updated by following the applying the chain rule to the expected return from the start distribution J with respect to the actor parameters:
    $$
    \begin{align*}
    âˆ‡_{Î¸^Âµ}J &â‰ˆ E_{s_tâˆ¼Ï^Î²} [âˆ‡_{Î¸^Âµ}Q(s, a|Î¸^Q)|_{s=s_t,a=Âµ(s_t|Î¸^Âµ)}]\\
    &= E_{s_tâˆ¼Ï^Î²}[ âˆ‡_aQ(s, a|Î¸^Q)|_{s=st,a=Âµ(s_t)}âˆ‡_{Î¸^Âµ}
    Âµ(s|Î¸^Âµ)|_{s=st}]
    \end{align*}
    $$

#### DDPGç®—æ³•

å¯¹äºQ-Learningè¿™æ ·çš„ç®—æ³•æ¥è¯´ï¼Œéçº¿æ€§å‡½æ•°é€¼è¿‘æ˜¯ä¸èƒ½ä¿è¯æ”¶æ•›æ€§çš„ï¼Œå› æ­¤å€Ÿé‰´DQNç”¨ç¥ç»ç½‘ç»œæ¥åœ¨çº¿é€¼è¿‘å‡½æ•°çš„æˆåŠŸæ¡ˆä¾‹ï¼Œå¼€å‘äº†DDPGã€‚

One challenge when using neural networks for reinforcement learning is that most optimization algorithms assume that the samples are independently and identically distributed(IID). **Additionally, to make efficient use of hardware optimizations, it is essential to learn in mini- batches, rather than online.**

1. Problem 1 å‘æ•£ï¼š

   Since the network $Q(s, a|Î¸^Q)$ being updated is also used in calculating the target value (equation 5), the Q update is prone to divergence. We create a copy of the actor and critic networks, $Qâ€˜(s, a|Î¸^{Qâ€™})$
    and $Âµ'(s|Î¸^{Âµ'})$ respectively, that are used for calculating the target values. This means that the target values are constrained to change slowly, greatly improving the stability of learning. å³ä½¿å»¶è¿Ÿæ›´æ–°å¯¼è‡´äº†å­¦ä¹ é€Ÿåº¦æ…¢ï¼Œä½†æ˜¯å®ƒæ‰€å¸¦æ¥çš„ç¨³å®šæ€§æ›´åŠ é‡è¦ã€‚

2. Problem 2 è¾“å…¥é‡èŒƒå›´ä¸ä¸€è‡´ï¼š

   Using *batch normalization* (Ioffe & Szegedy, 2015). This technique normalizes each dimension across the samples in a minibatch to have unit mean and variance. In addition, it maintains a run- ning average of the mean and variance to use for normalization during testing.

3. Problem 3 æ¢ç´¢ï¼š

   DDPGè¿™æ ·çš„ç¦»çº¿å­¦ä¹ ç®—æ³•çš„å¥½å¤„åœ¨äºï¼Œå¯ä»¥å°†æ¢ç´¢ç‹¬ç«‹äºå­¦ä¹ ç®—æ³•ä¹‹å¤–ã€‚æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåŒ…å«å™ªå£°çš„åŠ¨ä½œç­–ç•¥ï¼š
   $$
   \mu'(s_t)=\mu(s_t|\theta_t^{\mu})+\mathcal{N}
   $$
   åˆ©ç”¨Ornstein-Uhlenbeckè¿‡ç¨‹æ¥ç”Ÿæˆæƒ¯æ€§ç‰©ç†ç³»ç»Ÿçš„æ¢ç´¢

![1558614556514](./img/1558614556514.png)

#### ç›¸å…³é—®é¢˜

**Trust region policy optimization (TRPO)** (Schulman et al., 2015b), directly constructs stochastic neural network policies without decomposing problems into optimal control and supervised phases. ç»è¿‡ç²¾å‡†çš„è°ƒå‚ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿‘ä¹å•è°ƒçš„æ›´æ–°ï¼Œå¹¶ä¸”ä¸éœ€è¦è®¡ç®—åŠ¨ä½œå€¼å‡½æ•°ï¼Œå¯èƒ½å› æ­¤å¯¼è‡´æ•°æ®æ•ˆç‡ä¸‹é™ã€‚

**Guided policy search (GPS) ** algorithms (e.g., (Levine et al., 2015)) decomposes the problem (Actor-Criticå­˜åœ¨çš„é—®é¢˜) into three phases that are rela- tively easy to solve: 

1. First, it uses full-state observations to create locally-linear approximations of the dynamics around one or more nominal trajectories, 
2. Then uses optimal control to find the locally-linear optimal policy along these trajectories; 
3. Finally, it uses supervised learning to train a complex, non-linear policy (e.g. a deep neural network) to reproduce the state-to-action mapping of the optimized trajectories.

**PILCO** (Deisenroth & Rasmussen, 2011) uses **Gaussian processes** to learn a non-parametric, probabilistic model of the dynamics. ä½†æ˜¯åœ¨é«˜ç»´é—®é¢˜ä¸Šä¸åˆ‡å®é™…ã€‚**ä¸è¿‡æ·±åº¦å‡½æ•°é€¼è¿‘çœ‹èµ·æ¥æ˜¯å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨åˆ°å¤§å‹çš„ã€é«˜ç»´é—®é¢˜ä¸Šçš„æœ€å¥½åŠæ³•ã€‚**

è¿˜å¯ä»¥ä½¿ç”¨**å‹ç¼©æƒé‡**æˆ–**æ— ç›‘ç£å­¦ä¹ **çš„æ–¹æ³•æ¥ä»åƒç´ ä¸­å­¦ä¹ ç­–ç•¥(KoutnÂ´Ä±k et al., 2014) ï¼ˆä¸¤ç¯‡ï¼‰

#### ä¸è¶³

éœ€è¦å¤§é‡çš„è®­ç»ƒæ­¥æ•°æ‰èƒ½æ‰¾åˆ°è§£å†³æ–¹æ¡ˆã€‚

